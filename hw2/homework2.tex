\documentclass[12pt]{article}
\usepackage{fullpage,mathpazo,amsfonts,nicefrac,amsmath,amsthm}

\newtheorem{fact}{Fact}
\newtheorem*{lemma}{Lemma}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\Tr}{tr}

\begin{document}

\section*{CMPS 142: Homework 2}

\begin{tabular}{p{8cm} l l}
    & Dominic Balassone & dbalasso@ucsc.edu \\
    & Duncan McColl & dmccoll@ucsc.edu\\
    & Kevin Doyle 	& kdoyle@ucsc.edu \\
\end{tabular}
    
\begin{enumerate}
  \item Weka Experiements
  
  \begin{enumerate}
  	\item[(a)] Comparing algorithms, testing on training set: \\ \\
  		\begin{tabular}{l l}
  			\textbf{Classifier} & \textbf{\% Correctly Classified Instances} \\
  			Nearest Neighbors & 100.0 \\
  			Naive Bayes & 76.3 \\
  			Logistic Regression & 78.3
  	
  		\end{tabular} \\
  		
  		Here, judging only by accuracy we see that nearest neighbors performed the best. This is because nearest neighbors compares the original training data with each test instance, and since each test instance is also an instance in the training data, it was an easy match. This demonstrates nearest neighbors as a good choice for when training data can account for the entire outcome space. 
  		
  		For practical purposes, it would be wiser to choose logistic regression as the 'best' algorithm, since it scored the highest with a non-trivial evaluation.
  		
  	\item[(b)] Logistic regression model: \\ \\
	  	\begin{math}
			\left(
	  			\begin{array}{r}
	  				-0.1232 \\
	  				-0.0352 \\
	  				0.0133 \\
	  				-0.0006 \\
	  				0.0012 \\
	  				-0.0897 \\
	  				-0.9452 \\
	  				-0.0149 
	  			\end{array}  		
	  		\right)
	  		\bullet
	  		\left(
	  			\begin{array}{r}
	  				preg \\
	  				plas \\
	  				pres \\
	  				skin \\
	  				insu \\
	  				mass \\
	  				pedi \\
	  				age 
	  			\end{array}
	  		\right)
	  		+ 8.4047 = 0	  		
	  	\end{math} \\
	\item[(c)] Comparing algorithms using 10-fold cross-validation: \\ \\
  		\begin{tabular}{l l}
  			\textbf{Classifier} & \textbf{\% Correctly Classified Instances} \\
  			Nearest Neighbors & 70.2 \\
  			Naive Bayes & 76.3 \\
  			Logistic Regression & 77.2
  		\end{tabular} \\
  		
  		Using cross-validation decreased the nearest neighbors accuracy significantly. The other two stayed pretty much the same. 
  		
  	\item[{d}] Normalize: \\
	  	The normalize filter transforms all the data values into the range [0, 1], using the formula $X - X_{min} / X_{max} - X_{min}$. This maintains the scale of each attribute within the unit interval.  Because of this logistic regression's prediction accuracy will not change.  \\
		The weight vector did change. Each new value in the weight vector is proportional to the old value with a factor of 1/range, $W_{new} = W_{old} * 1/range$. This is because the weight vector value describes how a unit of change in that value affects the probability of the outcome, and we have simply redefined a unit of change. \\	 
	 \item[(e)] Ridge Parameter: \\
	 	  		\textbf{0.0}: Accuracy is 77.2\% correctly classified. The weights are the same as when the ridge parameter was very very small, essentially zero anyway. \\
	 	  		\textbf{0.3}: Accuracy is 77.2\% correctly classified. The weights decrease a little, which is expected because a penalty on increasing the weights was introduced. However, the weights are similar enough that the accuracy was not affected. \\
	 	  		\textbf{2.0}: Accuracy is 77.1\%. Increasing the ridge value to 2.0 was enough to have a small affect on the accuracy. 
	 \item[(f)] KNN: \\
	 			\textbf{Prediction}: 3NN or 5NN will perform better than 1NN. With this data set, most of the attributes have ranges in which data with different classifications overlap. With 1NN, an outlier data point could easily cause the misclassification of test data, simply by the grace of that outlier being the closest to the test data. By basing classification on a larger number of neighbors, the 3NN and 5NN algorithms reduce the influence of outlier data points.  \\
	 			\textbf{Results}: 1NN had 70.2\% accuracy. 3NN had 72.7\% accuracy. 5NN had 73.2\% accuracy.  \\
	 \item[(g)] Copies of \textit{pres}: \\
	 	\textbf{Expected}:  For nearest neighbors, adding copy's of an attribute weights the distance from that attribute on a training instance to a test instance heavier, so the algorithm will favor instances who are closer in that attribute. This will affect the prediction and therefore the accuracy. If somehow we were to get lucky and the attribute we picked was more accurate and more fully characterized than the others, this could increase accuracy, otherwise you would expect it to go down.\\
	 	For Naive Bayes something similar will happen, that attribute will over contribute to the evidence portion of the posterior probability, ie. the normalizing constantant, so it will lessen the affect of the other variables in the decision making process. \\
	    For logistic regression the addition of the data will not affect the accuracy. The algorithm will essentially divide up the optimal weight for that attribute among the copies which will result the same prediction. \\
	 	\textbf{Results}: \\
  		\begin{tabular}{l l}
  			\textbf{Classifier} & \textbf{\% Correctly Classified Instances} \\
  			Nearest Neighbors & 67.3 \\
  			Naive Bayes & 71.6 \\
  			Logistic Regression & 77.2
  		\end{tabular} \\
  		
	\item[(h)] 20 random valued attributes: \\
  		\begin{tabular}{l l}
  			\textbf{Classifier} & \textbf{\% Correctly Classified Instances} \\
  			Nearest Neighbors & 58.7 \\
  			Naive Bayes & 75.0 \\
  			Logistic Regression & 76.0
  		\end{tabular} \\
  	
  \end{enumerate}
  
  \item Bayesian Probability \\
  	\begin{enumerate}
  		\item[(a)] Outcome space: $\{FF, FM, MF, MM\}$, and an atomic event is any one of those outcomes. 
  		\item[(b)] Using Bayes rule, we have \\
	  		\begin{align*}
	  			 P(Exactly 1 Female &\mid Some Male ) \\  &= \frac{P( Some Male \mid Exactly 1 Female) P(Exactly 1 Female)}{P(Some Male)} \\
	  			 & = \frac{1 \cdot \frac{1}{2}}{\frac{3}{4}} = \frac{2}{3}
	  		\end{align*} 
	  		Where it can be seen from the outcome space that when there is exactly one female child, there is a 100\% probability that the second child is male. There is a 50\% chance of having exactly one female child, because half of the possible outcomes include a single female child. Finally, three of the outcomes include a male child. \\
		\item[(c)] Having seen that the younger child is a boy, using Bayes rule, we have
	  		\begin{align*}
	  			 P( Older & Child Female \mid Younger Child Male ) \\
	  			  &= \frac{P( Younger Child Male \mid Older Child Female ) P(Older Child Female)}{P(Younger Child Male)} \\
	  			 & = \frac{1 \cdot \frac{1}{2}}{1} = \frac{1}{2}
	  		\end{align*} 
	  		The key here is with $P( Younger Child Male \mid Older Child Female )$. We saw that the younger child is male, and so if we are given that the older child is female, we already know without a doubt that the younger is male. In the outcome space there are two possible events which include an older female, so $P(Older Child Female)$ is $\frac{1}{2}$. We saw the boy so $P(Younger Child Male)$ is $1$.
	  	 
	  		
  	\end{enumerate}
  	
  	\item Naive Bayes \\
  		In order to predict Honors status, $H$, for students who have taken AP courses we look at 
  		\begin{align*}
  			P(H \mid AP, GPA) &= \frac{P(AP \mid H) P(GPA \mid H) P(H)}{P(AP,GPA)}.
  		\end{align*}
  		Specifically, in order to predict $H$ from GPA data, we find the GPA values which make the following true.
  		\begin{align*}
  		\frac{P(AP \mid \overline{H}) P(GPA \mid \overline{H}) P(\overline{H})}{P(AP,GPA)} < \frac{P(AP \mid H) P(GPA \mid H) P(H)}{P(AP,GPA)}
  		\end{align*} 
  		In solving for the GPA value, we can multiply both side of the inequality by the shared denominator and get the same GPA range by looking at 
  		\begin{align*}
  		P(AP \mid \overline{H}) P(GPA \mid \overline{H}) P(\overline{H}) < P(AP \mid H) P(GPA \mid H) P(H) \Longleftrightarrow \\
  		0 < P(AP \mid H) P(GPA \mid H) P(H) - \left( P(AP \mid \overline{H}) P(GPA \mid \overline{H}) P(\overline{H}) \right) 
  		\end{align*}
  		The following table can be used to fill out and solve this inequality.\\ \\
  		\begin{tabular}{l l}
  			\textbf{Expression} & \textbf{Equivalent} \\
  			$P(AP \mid H)$ & $2/3$ \\
  			$P(AP \mid \overline{H})$ & $2/6$ \\
  			$P(H)$ & $3/9$ \\
  			$P(\overline{H})$ & $5/9$ \\
  			$P(GPA \mid H)$ & $\frac{1}{\sqrt{2\pi\sigma_{H}^2}} e^{- \frac{(GPA - \mu_{H})^2}{2\sigma_{H}^2}} $ \\  	
  			$\mu_{H}$ & $3.4$ \\		
  			$\sigma_{H}^2$ & $0.42$ \\
  			$P(GPA \mid \overline{H})$ & $\frac{1}{\sqrt{2\pi\sigma_{\overline{H}}^2}} e^{- \frac{(GPA - \mu_{\overline{H}})^2}{2\sigma_{\overline{H}}^2}} $ \\
  			$\mu_{\overline{H}}$ & $3.0$ \\
  			$\sigma_{\overline{H}}^2$ & $0.24333$ \\
  		\end{tabular} \\ \\
  		After filling out and solving the inequality for GPA, we find that we can predict: \\
  		If AP courses are taken, predict $H$ if the GPA is between $1.6783$ and $3.2198$. \\ 
  		
  		Evaluating honors status when AP courses are not taken can be done with very similar calculations, however $AP$ need to be replaced with $\overline{AP}$ and the following values should be used: \\ \\
  		\begin{tabular}{l l}
			\textbf{Expression} & \textbf{Equivalent} \\
	 		$P(\overline{AP} \mid H )$ & $1/3$ \\
	 		$P(\overline{AP} \mid \overline{H} )$ & $4/6$ \\
  		\end{tabular} \\ \\
  		Now we can say: \\
  		If AP courses are not taken, predict $H$ if the GPA is between $1.7583$ and $3.1398$. \\ \\

  		
  	\item Expected Value \\
	  	\begin{align*}
		    E[XY] &= \sum_{X}\sum_{Y}x_i y_iP(x_i,y_i) \\
		  	&= \sum_{X}\sum_{Y}x_i y_iP(x_i)P(y_i) \\
		  	&= \sum_{X}x_i P(x_i)\sum_{Y}y_i P(y_i) \\
		  	&= E[X]E[Y]
	  	\end{align*}
  	
\end{enumerate}

\end{document}
