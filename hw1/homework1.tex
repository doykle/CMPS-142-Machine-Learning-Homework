\documentclass[12pt]{article}
\usepackage{fullpage,mathpazo,amsfonts,nicefrac,amsmath,amsthm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\section*{CMPS 142: Homework 1}

\begin{tabular}{p{10cm} l}
    & Alejandro Aguilar \\
    & John Michael \\
    & Kevin Doyle \\
\end{tabular}
    
\begin{enumerate}
  \item We will prove that for an arbitrary number of examples $m$, and number of features $n$, that the Least Sqares cost function $J(\theta)$ is a convex function of the $n-$dimensional parameter vector $\theta$.
  \item Weka Problem
  	\begin{enumerate}
  	\item[(a)] Model: $y= -0.1343(x_1) + 1.8477(x_2) + -0.8966(x_3) + 4.3608$ \\
  	Root mean squared error: $0.1897$
  	\item[(b)] For $\textbf{x} = [3,3,5]$, using the model from (a) we have $\text{\^{t}} = 5.018$
  	\item[(c)] $\theta = (4.3608, -0.1343, 1.8477, -0.8966)$ 
  	\item[(d)] As long as each instance $i$, $x^{(i)}$, has the same row index as the corresponding $y^{(i)}$, the data will be the same, and so the resulting regression model will be the same. 
  	\end{enumerate}
  \item To prove $ w \cdot x = \ln\big(\frac{p(1\mid x,w)}{p(0\mid x,w)}\big) $ we will simplify $\ln\big(\frac{p(1\mid x,w)}{p(0\mid x,w)}\big)$ and demonstrate the equality.
  
   \begin{proof}We were given $p(1\mid x,w) =\frac{e^{w \cdot x}}{1 + e^{w \cdot x}}$ and $p(0\mid x,w) = 1 - p(1\mid x,w)$. From this we can find that 
    
	\begin{align*}
	\begin{split}
	p(0\mid x,w) &= 1 - p(1\mid x,w) \\
	&= 1 - \frac{e^{w \cdot x}}{1 + e^{w \cdot x}} \\
	\end{split}
	\end{align*}
   
  
   \end{proof}
\end{enumerate}

\end{document}
